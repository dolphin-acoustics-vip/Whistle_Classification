{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer_Learning_Dolphin_Acoustics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook uses tranfer learning to classify spectrogram data on dolphin whistles. Much of the code used here was adpapted from https://keras.io/guides/transfer_learning/.\n",
        "\n",
        "Credit for parts of the code dealing with the `ImageDataGenerator` is given to Josh Wheeler and Gemma Ruseva (https://github.com/JoshWheeler08/DolphinAcoustics-Classifier/blob/main/vip_dolphin.ipynb)\n",
        "\n",
        "The code seeks to make a model which can distinguish among Common, Melon Head, and Bottlenose dolphin species."
      ],
      "metadata": {
        "id": "wNpTm6ROgBl1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDOl4Q4DGTbT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# NOTE: Although not ideal, transfer learning models should be saved in an h5 format to avoid issues with tensorflow loading.\n",
        "#       This (to date) seems to be the best workaround to an issue in tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive so data can be accessed\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # '/content' is the current working directory"
      ],
      "metadata": {
        "id": "zYtXIJi-NK9v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67b1243-25b0-4099-cf9a-12b6d2856578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Base Model and Adding Extra Layers\n",
        "\n",
        "Here we load the Xception base model for transfer learning.This is a fairly complex CNN-based model. More can be read about it here: https://arxiv.org/abs/1610.02357.\n",
        "\n",
        "The base model is \"frozen\" so that its hyperparameters are not drastically changed by subsequent training. \n",
        "\n",
        "We then add two new layers for training on the spectrogram data.\n",
        "\n",
        "A list of several alternatives to the Xception base model can be found here: https://keras.io/api/applications/#available-models. "
      ],
      "metadata": {
        "id": "44Aew7Y5e0qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# input shape set to that used with the dolphins spectrogram data\n",
        "image_shape = (202, 413, 3)\n",
        "\n",
        "base_model = keras.applications.Xception(\n",
        "    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n",
        "    input_shape=image_shape, \n",
        "    include_top=False,\n",
        ")  # Do not include the ImageNet classifier at the top.\n",
        "\n",
        "# Freeze the base_model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top\n",
        "inputs = keras.Input(shape=image_shape)\n",
        "x = keras.Sequential()(inputs)\n",
        "\n",
        "# Pre-trained Xception weights require that input be scaled\n",
        "# from (0, 255) to a range of (-1., +1.), the rescaling layer\n",
        "# outputs: `(inputs * scale) + offset`\n",
        "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
        "x = scale_layer(x)\n",
        "\n",
        "# The base model contains batchnorm layers. We want to keep them in inference mode\n",
        "# when we unfreeze the base model for fine-tuning, so we make sure that the\n",
        "# base_model is running in inference mode here.\n",
        "x = base_model(x, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
        "\n",
        "number_of_outputs = 3 # Here, the number of outputs is set to three since we are working \n",
        "                      # on a three-category multi-classification problem.\n",
        "outputs = keras.layers.Dense(number_of_outputs)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ujt1oXivG1W9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ae67e8f-a558-4db8-d684-43b9a0b80f1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "83697664/83683744 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 202, 413, 3)]     0         \n",
            "                                                                 \n",
            " sequential (Sequential)     multiple                  0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 202, 413, 3)       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 7, 13, 2048)       20861480  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,867,627\n",
            "Trainable params: 6,147\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Training, Test and Validation Data\n",
        "\n",
        "The spectrogram image data are now loaded."
      ],
      "metadata": {
        "id": "4z-YiiS1iMO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code section adapted from https://github.com/JoshWheeler08/DolphinAcoustics-Classifier/blob/main/vip_dolphin.ipynb\n",
        "\n",
        "IMAGE_SHAPE = (202, 413)\n",
        "directory_name = \"/content/drive/MyDrive/Dolphin_Acoustics_VIP/normalised-dclde-clips/train-test/\"\n",
        "\n",
        "TEST_DATA_DIR = directory_name + \"test\"\n",
        "TRAINING_DATA_DIR = directory_name + \"train\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=.20\n",
        ") #https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
        "\n",
        "test_generator = ImageDataGenerator(\n",
        "                    rescale=1./255\n",
        "                ).flow_from_directory(\n",
        "                      TEST_DATA_DIR,\n",
        "                      shuffle=True,\n",
        "                      batch_size = 50,\n",
        "                      target_size=IMAGE_SHAPE\n",
        "                      )\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR,\n",
        "    subset=\"validation\",\n",
        "    shuffle=True,\n",
        "    target_size=IMAGE_SHAPE\n",
        ")\n",
        "                \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAINING_DATA_DIR,\n",
        "    subset=\"training\",\n",
        "    shuffle=True,\n",
        "    target_size=IMAGE_SHAPE\n",
        ")"
      ],
      "metadata": {
        "id": "Z810mkC7La07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31fcf92c-0e9e-48e1-9535-74abf2db2e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2557 images belonging to 3 classes.\n",
            "Found 1193 images belonging to 3 classes.\n",
            "Found 4777 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the Model to the Spectrogram image data.\n",
        "\n",
        "We now compile and fit the model. The new classification layers are trained, while the base model's hyperparameters remain unchanged."
      ],
      "metadata": {
        "id": "7HWnD4YCigQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(),\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "epochs = 15\n",
        "\n",
        "model.fit(test_generator, epochs=epochs, validation_data=validation_generator)\n",
        "model.summary()\n",
        "model.save(directory_name + \"2022_04_04_xception.h5\")"
      ],
      "metadata": {
        "id": "G9jP2AUsG2tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "242acc28-e15d-47ba-f0c7-b8ea6f2d833b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "52/52 [==============================] - 968s 18s/step - loss: 1.0844 - categorical_accuracy: 0.3880 - val_loss: 1.0643 - val_categorical_accuracy: 0.4359\n",
            "Epoch 2/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0563 - categorical_accuracy: 0.4431 - val_loss: 1.0428 - val_categorical_accuracy: 0.4803\n",
            "Epoch 3/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0316 - categorical_accuracy: 0.5084 - val_loss: 1.0207 - val_categorical_accuracy: 0.5616\n",
            "Epoch 4/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 1.0095 - categorical_accuracy: 0.5264 - val_loss: 1.0023 - val_categorical_accuracy: 0.5725\n",
            "Epoch 5/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9976 - categorical_accuracy: 0.5350 - val_loss: 0.9866 - val_categorical_accuracy: 0.5809\n",
            "Epoch 6/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9825 - categorical_accuracy: 0.5389 - val_loss: 0.9757 - val_categorical_accuracy: 0.5608\n",
            "Epoch 7/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9675 - categorical_accuracy: 0.5522 - val_loss: 0.9595 - val_categorical_accuracy: 0.5616\n",
            "Epoch 8/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9549 - categorical_accuracy: 0.5690 - val_loss: 0.9515 - val_categorical_accuracy: 0.5549\n",
            "Epoch 9/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9493 - categorical_accuracy: 0.5526 - val_loss: 0.9369 - val_categorical_accuracy: 0.5801\n",
            "Epoch 10/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9376 - categorical_accuracy: 0.5679 - val_loss: 0.9275 - val_categorical_accuracy: 0.5809\n",
            "Epoch 11/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9329 - categorical_accuracy: 0.5675 - val_loss: 0.9231 - val_categorical_accuracy: 0.5490\n",
            "Epoch 12/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9189 - categorical_accuracy: 0.5878 - val_loss: 0.9153 - val_categorical_accuracy: 0.5708\n",
            "Epoch 13/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9141 - categorical_accuracy: 0.5894 - val_loss: 0.9028 - val_categorical_accuracy: 0.6027\n",
            "Epoch 14/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9106 - categorical_accuracy: 0.5937 - val_loss: 0.8956 - val_categorical_accuracy: 0.5968\n",
            "Epoch 15/15\n",
            "52/52 [==============================] - 61s 1s/step - loss: 0.9013 - categorical_accuracy: 0.6117 - val_loss: 0.8938 - val_categorical_accuracy: 0.5859\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 202, 413, 3)]     0         \n",
            "                                                                 \n",
            " sequential (Sequential)     multiple                  0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 202, 413, 3)       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 7, 13, 2048)       20861480  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,867,627\n",
            "Trainable params: 6,147\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning the Model\n",
        "\n",
        "Now that we have trained our own classification layers on top of the Xception base model, we can train all layers of the model now by setting `base_model.trainable = True` (i.e. \"unfreezing\" the base model) and setting a very low learning rate. A low learning rate is used to prevent destroying the model's useful pre-trained features.\n",
        "\n",
        "Doing this would ideally provide an extra boost to the model's performance, but it can lead to overfitting if we are not careful."
      ],
      "metadata": {
        "id": "3eBprpC3lUV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the base_model. Note that it keeps running in inference mode\n",
        "# since we passed `training=False` when calling it. This means that\n",
        "# the batchnorm layers will not update their batch statistics.\n",
        "# This prevents the batchnorm layers from undoing all the training\n",
        "# we've done so far.\n",
        "base_model.trainable = True\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n",
        "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[keras.metrics.CategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "epochs = 10\n",
        "model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
        "\n",
        "# save final fine-tuned model\n",
        "model.save(directory_name + \"2022_04_04_xception_fine_tuned.h5\")"
      ],
      "metadata": {
        "id": "C3tzX1iLdi3j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb21f0e-cd4a-4578-f101-5c4b383bde8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 202, 413, 3)]     0         \n",
            "                                                                 \n",
            " sequential (Sequential)     multiple                  0         \n",
            "                                                                 \n",
            " rescaling (Rescaling)       (None, 202, 413, 3)       0         \n",
            "                                                                 \n",
            " xception (Functional)       (None, 7, 13, 2048)       20861480  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 6147      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,867,627\n",
            "Trainable params: 20,813,099\n",
            "Non-trainable params: 54,528\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "150/150 [==============================] - 1529s 10s/step - loss: 0.8237 - categorical_accuracy: 0.6255 - val_loss: 0.5675 - val_categorical_accuracy: 0.7687\n",
            "Epoch 2/10\n",
            "150/150 [==============================] - 362s 2s/step - loss: 0.5096 - categorical_accuracy: 0.8018 - val_loss: 0.4217 - val_categorical_accuracy: 0.8215\n",
            "Epoch 3/10\n",
            "150/150 [==============================] - 362s 2s/step - loss: 0.4013 - categorical_accuracy: 0.8461 - val_loss: 0.4076 - val_categorical_accuracy: 0.8349\n",
            "Epoch 4/10\n",
            "150/150 [==============================] - 362s 2s/step - loss: 0.3484 - categorical_accuracy: 0.8690 - val_loss: 0.3266 - val_categorical_accuracy: 0.8667\n",
            "Epoch 5/10\n",
            "150/150 [==============================] - 362s 2s/step - loss: 0.3097 - categorical_accuracy: 0.8857 - val_loss: 0.3333 - val_categorical_accuracy: 0.8625\n",
            "Epoch 6/10\n",
            "150/150 [==============================] - 363s 2s/step - loss: 0.2366 - categorical_accuracy: 0.9131 - val_loss: 0.2964 - val_categorical_accuracy: 0.8902\n",
            "Epoch 7/10\n",
            "150/150 [==============================] - 362s 2s/step - loss: 0.2464 - categorical_accuracy: 0.9060 - val_loss: 0.3927 - val_categorical_accuracy: 0.8424\n",
            "Epoch 8/10\n",
            "150/150 [==============================] - 364s 2s/step - loss: 0.2003 - categorical_accuracy: 0.9253 - val_loss: 0.2858 - val_categorical_accuracy: 0.8944\n",
            "Epoch 9/10\n",
            "150/150 [==============================] - 364s 2s/step - loss: 0.1669 - categorical_accuracy: 0.9391 - val_loss: 0.2652 - val_categorical_accuracy: 0.9053\n",
            "Epoch 10/10\n",
            "150/150 [==============================] - 365s 2s/step - loss: 0.1505 - categorical_accuracy: 0.9495 - val_loss: 0.3599 - val_categorical_accuracy: 0.8785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation\n",
        "\n",
        "We now do some elementary analysis on the performance of our model on test data. See https://github.com/dolphin-acoustics-vip/CetaceXplain for how we can use CetaceXplain for more in-depth analysis"
      ],
      "metadata": {
        "id": "I42kLcc9MHkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code section adapted from https://github.com/JoshWheeler08/DolphinAcoustics-Classifier/blob/main/vip_dolphin.ipynb\n",
        "test_generator.reset()\n",
        "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
        "\n",
        "# SUMMARY STATISTICS\n",
        "print(\"----- Evaluation Summary statistics -----\")\n",
        "print(\"Test accuracy = \", test_acc)\n",
        "print(\"Test loss = \", test_loss)"
      ],
      "metadata": {
        "id": "j_aezGWqXFPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1d4db63-0b34-4a42-a293-1d16f919bac6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52/52 - 40s - loss: 0.2685 - categorical_accuracy: 0.9054 - 40s/epoch - 771ms/step\n",
            "----- Evaluation Summary statistics -----\n",
            "Test accuracy =  0.905357837677002\n",
            "Test loss =  0.2684841752052307\n"
          ]
        }
      ]
    }
  ]
}
